---
title: " Machine Learning: The Prediction of Customer Churn with Keras"
date: 2018-01-18
excerpt: "In this post, the prediction of the customer churn is implemented with Keras, and the performance of difference optimizers in ANN is investigated."
mathjax: "true"
---
## 1. Introduction
Customer churn refers to when a customer is likely to cancel a subscription, product or service. Since it is always harder and expensive to acquire a new customer than it is to retain a current paying customer, the churn prediction is essential for businesses. In this post, a churn prediction is an implementation by Artificial Neuro Network(ANN). 
 
<img src="{{ site.url }}{{ site.baseurl }}/images/ml_churn/1_1.png" alt="linearly separable data">

## 2. Dataset
The datasets are offered by a bank with customer information and there are thirteen items may impact the customer churn. From the following figures, we could find that the dataset is balanced, the customer churning rate is approximately 10%.
 
<img src="{{ site.url }}{{ site.baseurl }}/images/ml_churn/2_2.png" alt="linearly separable data">

## 3. Modeling and Result
Before modeling, a data preprocess pipeline is designed including filling missing value, shuffling and feature scaling using Python Scikit-learn. Then the Artificial Neuro Network (ANN) is implemented with Keras, and the accuracy is of 86%. In the Python code, the number of an epoch is set to 100 and work well in this case. However, we may face a problem when the size of dataset expands dramatically. So, the performances of different optimizer are estimated in next section.

<img src="{{ site.url }}{{ site.baseurl }}/images/ml_churn/3_1.png" alt="linearly separable data">

```python
# Importing the Keras libraries and packages
import keras
from keras.models import Sequential
from keras.layers import Dense

# Initialising the ANN
classifier = Sequential()

# Adding the input layer and the first hidden layer
classifier.add(Dense(output_dim = 6, init = 'uniform', activation = 'relu', input_dim = 11))

# Adding the second hidden layer
classifier.add(Dense(output_dim = 6, init = 'uniform', activation = 'relu'))

# Adding the output layer
classifier.add(Dense(output_dim = 1, init = 'uniform', activation = 'sigmoid'))

# Compiling the ANN
classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])

# Fitting the ANN to the Training set
classifier.fit(X_train, y_train, batch_size = 10, nb_epoch = 100)
```

## 4. Optimizer Comparison
In this project, different optimizers have slimier accuracy, so the converging rate is paid attention in comparison. In the Neural Network, the Gradient Descent is majorly used for weights updating with slow performance. The Stochastic Gradient Descent(SGD) performs a parameter update for each training example and is faster than the Gradient Descent. However, the complicated convergence due to frequent updates may result in high variance oscillations and slow convergence. The Momentum can soften the oscillations in irrelevant directions and keep going along the relevant direction and has better performance. With considering adaptive learning rates for each parameter, Adaptive Moment Estimation (Adam) is outperforming. It has not only good learning rate, fast convergence, but also low variance in the parameter updates.
 
<img src="{{ site.url }}{{ site.baseurl }}/images/ml_churn/4_1.png" alt="linearly separable data">

## 4. Conclusion
In this post, an Artificial Neuron Network is designed to predict customer churn with an accuracy of 86%. As the comparison among different optimizers, the optimizer of ‘Adam’ outperforms other optimizers.
 
## Reference
Anish Singh Walia, 2017. Types of Optimization Algorithms used in Neural Networks and Ways to Optimize Gradient Descent. [link](https://towardsdatascience.com/types-of-optimization-algorithms-used-in-neural-networks-and-ways-to-optimize-gradient-95ae5d39529f)
